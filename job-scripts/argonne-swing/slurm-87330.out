/home/ac.gwilkins/.conda/envs/fedsz2/lib/python3.11/site-packages/transformers/utils/hub.py:122: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Downloading shards:   0%|          | 0/19 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 19/19 [00:00<00:00, 1364.26it/s]
Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/19 [00:11<03:31, 11.76s/it]Loading checkpoint shards:  11%|█         | 2/19 [00:24<03:32, 12.48s/it]Loading checkpoint shards:  16%|█▌        | 3/19 [00:37<03:21, 12.60s/it]Loading checkpoint shards:  21%|██        | 4/19 [00:49<03:05, 12.36s/it]Loading checkpoint shards:  26%|██▋       | 5/19 [01:02<02:55, 12.57s/it]Loading checkpoint shards:  32%|███▏      | 6/19 [01:15<02:44, 12.65s/it]Loading checkpoint shards:  37%|███▋      | 7/19 [01:21<02:08, 10.67s/it]Loading checkpoint shards:  42%|████▏     | 8/19 [01:22<01:22,  7.48s/it]Loading checkpoint shards:  47%|████▋     | 9/19 [01:23<00:54,  5.40s/it]Loading checkpoint shards:  53%|█████▎    | 10/19 [01:23<00:34,  3.89s/it]Loading checkpoint shards:  58%|█████▊    | 11/19 [01:24<00:23,  2.93s/it]Loading checkpoint shards:  63%|██████▎   | 12/19 [01:25<00:15,  2.25s/it]Loading checkpoint shards:  68%|██████▊   | 13/19 [01:25<00:10,  1.77s/it]Loading checkpoint shards:  74%|███████▎  | 14/19 [01:26<00:06,  1.38s/it]Loading checkpoint shards:  79%|███████▉  | 15/19 [01:27<00:04,  1.21s/it]Loading checkpoint shards:  84%|████████▍ | 16/19 [01:28<00:03,  1.13s/it]Loading checkpoint shards:  89%|████████▉ | 17/19 [01:28<00:01,  1.05it/s]Loading checkpoint shards:  95%|█████████▍| 18/19 [01:29<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 19/19 [01:29<00:00,  1.33it/s]Loading checkpoint shards: 100%|██████████| 19/19 [01:29<00:00,  4.73s/it]
WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
slurmstepd: error: *** JOB 87330 ON gpu6 CANCELLED AT 2024-01-25T20:58:53 DUE TO TIME LIMIT ***
