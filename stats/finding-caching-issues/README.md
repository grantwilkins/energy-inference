# Analysis of Caching Issues in Models

In this directory are results of the analysis of the caching issues in the models. We found that LLMs cache KV pairs and therefore this reduces the runtime of inference operations, but it also creates misleading profiles of runtime and energy consumption.
